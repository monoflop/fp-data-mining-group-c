{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abhängigkeiten\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz einlesen und verarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten laden\n",
    "#strip_1 enthält keine near Einträge mit 1.0?\n",
    "strip_train = pd.read_csv('data/train/strip_1_train.csv', sep=',')\n",
    "strip_test = pd.read_csv('data/test/strip_1_test_no_labels.csv', sep=',')\n",
    "strip_offset = 0\n",
    "#strip_train.head(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO simplify\n",
    "#Label aus Datensatz entfernen, damit wir einen Label-Satz und einen Feature-Satz erhalten.\n",
    "X_train = strip_train.drop('near',axis = 1)\n",
    "Y_train = strip_train['near']\n",
    "\n",
    "#Alle nicht Sensordaten entfernen\n",
    "X_train = X_train.drop('frame_number',axis = 1)\n",
    "X_train = X_train.drop('strip_id',axis = 1)\n",
    "X_train = X_train.drop('node_id',axis = 1)\n",
    "X_train = X_train.drop('timestamp',axis = 1)\n",
    "X_train = X_train.drop('run_number',axis = 1)\n",
    "X_train = X_train.drop('vicon_x',axis = 1)\n",
    "X_train = X_train.drop('vicon_y',axis = 1)\n",
    "\n",
    "#NaN Werte normalisieren\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "\n",
    "#Datenstruktur so ändern, dass wir pro frame eine Zeile mit 150 Sensordaten erhalten\n",
    "#Anzahl Zeilen / 15 = Frames\n",
    "frames = len(X_train.index) / 15\n",
    "\n",
    "trainingData = np.zeros((int(frames),150), dtype=np.float64)\n",
    "frame_number = 0\n",
    "node_index = 0\n",
    "for i, row in X_train.iterrows():\n",
    "    trainingData[frame_number][node_index * 10 + 0] = row['ax']\n",
    "    trainingData[frame_number][node_index * 10 + 1] = row['ay']\n",
    "    trainingData[frame_number][node_index * 10 + 2] = row['az']\n",
    "    trainingData[frame_number][node_index * 10 + 3] = row['gx']\n",
    "    trainingData[frame_number][node_index * 10 + 4] = row['gy']\n",
    "    trainingData[frame_number][node_index * 10 + 5] = row['gz']\n",
    "    trainingData[frame_number][node_index * 10 + 6] = row['mx']\n",
    "    trainingData[frame_number][node_index * 10 + 7] = row['my']\n",
    "    trainingData[frame_number][node_index * 10 + 8] = row['mz']\n",
    "    trainingData[frame_number][node_index * 10 + 9] = row['r']\n",
    "    node_index = node_index + 1\n",
    "    if(node_index >= 15):\n",
    "        frame_number = frame_number + 1\n",
    "        node_index = 0\n",
    "    \n",
    "#Die Labels Y_train auch auf jede 15te Zeile reduzieren\n",
    "trainingLabels = np.zeros((int(frames)), dtype=np.int64)\n",
    "for i, number in Y_train.iteritems():\n",
    "    if(i % 15 == 0):\n",
    "        trainingLabels[int(i / 15)] = int(number)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten einheitlich skalieren von z. B. 0.0 - 1.0\n",
    "sc = StandardScaler()\n",
    "sc.fit(trainingData)\n",
    "trainingData = sc.transform(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO simplify\n",
    "#Alle nicht Sensordaten entfernen\n",
    "X_test = strip_test.drop('frame_number',axis = 1)\n",
    "X_test = X_test.drop('strip_id',axis = 1)\n",
    "X_test = X_test.drop('node_id',axis = 1)\n",
    "X_test = X_test.drop('timestamp',axis = 1)\n",
    "\n",
    "#NaN Werte normalisieren\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "#Datenstruktur so ändern, dass wir pro frame eine Zeile mit 150 Sensordaten erhalten\n",
    "#Anzahl Zeilen / 15 = Frames\n",
    "testFrames = len(X_test.index) / 15\n",
    "\n",
    "testData = np.zeros((int(testFrames),150), dtype=np.float64)\n",
    "frame_number = 0\n",
    "node_index = 0\n",
    "for i, row in X_test.iterrows():\n",
    "    testData[frame_number][node_index * 10 + 0] = row['ax']\n",
    "    testData[frame_number][node_index * 10 + 1] = row['ay']\n",
    "    testData[frame_number][node_index * 10 + 2] = row['az']\n",
    "    testData[frame_number][node_index * 10 + 3] = row['gx']\n",
    "    testData[frame_number][node_index * 10 + 4] = row['gy']\n",
    "    testData[frame_number][node_index * 10 + 5] = row['gz']\n",
    "    testData[frame_number][node_index * 10 + 6] = row['mx']\n",
    "    testData[frame_number][node_index * 10 + 7] = row['my']\n",
    "    testData[frame_number][node_index * 10 + 8] = row['mz']\n",
    "    testData[frame_number][node_index * 10 + 9] = row['r']\n",
    "    node_index = node_index + 1\n",
    "    if(node_index >= 15):\n",
    "        frame_number = frame_number + 1\n",
    "        node_index = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten einheitlich skalieren von z. B. 0.0 - 1.0\n",
    "sc.fit(testData)\n",
    "testData = sc.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest\n",
    "forest = RandomForestClassifier(n_estimators=200, random_state = 0)\n",
    "forest.fit(trainingData, trainingLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_prediction = forest.predict(testData)\n",
    "\n",
    "f = open(\"data.csv\", \"w\")\n",
    "f.write(\"Id,Predicted\\n\")\n",
    "count = strip_offset\n",
    "for a in forest_prediction:\n",
    "    f.write(str(count))\n",
    "    f.write(\",\")\n",
    "    f.write(str(int(a)))\n",
    "    f.write(\"\\n\")\n",
    "    count = count + 1 \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
